---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

<div class="wordwrap">You can also find my articles on my <a href="{{site.author.googlescholar}}">Google Scholar profile</a>.<br>
In top venues: TPAMI × 3, CVPR × 5, ICCV × 1, ECCV × 1, MM × 1</div>

<h2><span>2024</span></h2>
  <div class="flex-row" onmouseout="par2net_stop()" onmouseover="par2net_start()">
    <div class="image-container">
      <img src="../images/color4E.png" width="160" alt="Image">
    </div>
    <div class="text-container">
      <papertitle>Color4E: Event demosaicing for full-color event guided image deblurring</papertitle>
      <br>
      Yi Ma<sup>#</sup>,<strong>Peiqi Duan<sup>#</sup></strong>, Yuchen Hong, Chu Zhou, Yu Zhang, Jimmy Ren, and Boxin Shi<sup>*</sup>
      <br>
      <em>ACM MM</em>, 2024
      <br>
    </div>
  </div>

  <div class="flex-row" onmouseout="par2net_stop()" onmouseover="par2net_start()">
    <div class="image-container">
      <img src="../images/EvDiG.png" width="160" alt="Image">
    </div>
    <div class="text-container">
      <papertitle>EvDiG: Event-guided direct and global components separation</papertitle>
      <br>
      Xinyu Zhou, <strong>Peiqi Duan</strong>, Boyu Li, Chu Zhou, Chao Xu, and Boxin Shi<sup>*</sup>
      <br>
      <em>CVPR</em>, 2024  [Received full marks]  [Oral presentation, 0.78%]
      <br>
      <a href="https://assets.ctfassets.net/yreyglvi5sud/4rrKmGuR98bvBBmLlZK7i3/7c2a00d49adde30a4caba7cdec852f24/Zhou_CVPR24.pdf">[paper]</a> 
      <a href="https://www.youtube.com/watch?v=y0bMZnUJt14">[video]</a>
    </div>
  </div>

  <div class="flex-row" onmouseout="par2net_stop()" onmouseover="par2net_start()">
    <div class="image-container">
      <img src="../images/EventAid.png" width="160" alt="Image">
    </div>
    <div class="text-container">
      <papertitle>EventAid: Benchmarking event-aided image/video enhancement algorithms with real-captured hybrid dataset</papertitle>
      <br>
      <strong>Peiqi Duan<sup>#</sup></strong>, Boyu Li<sup>#</sup>, Yixin Yang, Hanyue Lou, Minggui Teng, Yi Ma, Boxin Shi<sup>*</sup>
      <br>
      <em>Arxiv</em>, 2024
      <br>
      <a href="https://arxiv.org/pdf/2312.08220">[paper]</a> 
    </div>
  </div>

<h2><span>2023</span></h2>
  <div class="flex-row" onmouseout="par2net_stop()" onmouseover="par2net_start()">
    <div class="image-container">
      <img src="../images/HDRpami.png" width="160" alt="Image">
    </div>
    <div class="text-container">
      <papertitle>Hybrid high dynamic range imaging fusing neuromorphic and conventional images</papertitle>
      <br>
      Jin Han, Yixin Yang, <strong>Peiqi Duan</strong>, Chu Zhou, Lei Ma, Chao Xu, Tiejun Huang, Imari Sato, and Boxin Shi<sup>*</sup>
      <br>
      <em>TPAMI</em>, 2023
      <br>
      <a href="https://downloads.ctfassets.net/yreyglvi5sud/7yA8sqjDJilRmL5iseiRpB/734af0b70b6b8966d79ff4dd6d8cf73a/Han_TPAMI22.pdf">[paper]</a> 
      <a href="https://github.com/hjynwa/NeurImg-HDR">[website]</a>
    </div>
  </div>

  <div class="flex-row" onmouseout="par2net_stop()" onmouseover="par2net_start()">
    <div class="image-container">
      <img src="../images/NeuroZoom.png" width="160" alt="Image">
    </div>
    <div class="text-container">
      <papertitle>NeuroZoom: Denoising and super resolving neuromorphic events and spikes</papertitle>
      <br>
      <strong>Peiqi Duan</strong>, Yi Ma, Xinyu Zhou, Xinyu Shi, Zihao W. Wang, Tiejun Huang, and Boxin Shi<sup>*</sup>
      <br>
      <em>TPAMI</em>, 2023
      <br>
      <a href="https://downloads.ctfassets.net/yreyglvi5sud/CfT1NA9r1HNehoQqZyf5t/0f0e305cfd5b73471226f5ef1dfa52fe/Duan_TPAMI23_comp.pdf">[paper]</a> 
      <a href="https://github.com/hjynwa/NeurImg-HDR">[website]</a>
    </div>
  </div>

  <div class="flex-row" onmouseout="par2net_stop()" onmouseover="par2net_start()">
    <div class="image-container">
      <img src="../images/LowLight.png" width="160" alt="Image">
    </div>
    <div class="text-container">
      <papertitle>Coherent event guided low-light video enhancement</papertitle>
      <br>
      Jinxiu Liang, Yixin Yang, Boyu Li, <strong>Peiqi Duan</strong>, Yong Xu, and Boxin Shi<sup>*</sup>
      <br>
      <em>ICCV</em>, 2023
      <br>
      <a href="https://assets.ctfassets.net/yreyglvi5sud/4jmy8h8OR0kmr3Oj1YaOIf/94751ea976fee4b06d9127095ac0249c/Liang_ICCV23a.pdf">[paper]</a> 
      <a href="https://sherrycattt.github.io/EvLowLight/">[website]</a>
    </div>
  </div>

  <div class="flex-row" onmouseout="par2net_stop()" onmouseover="par2net_start()">
    <div class="image-container">
      <img src="../images/EvPlug.png" width="160" alt="Image">
    </div>
    <div class="text-container">
      <papertitle>EvPlug: Learn a plug-and-play module for event and image fusion</papertitle>
      <br>
      Jianping Jiang, Xinyu Zhou, <strong>Peiqi Duan</strong>, Boxin Shi<sup>*</sup>
      <br>
      <em>Arxiv</em>, 2023
      <br>
      <a href="https://arxiv.org/pdf/2312.16933">[paper]</a> 
    </div>
  </div>

<h2><span>2022</span></h2>
  <div class="flex-row" onmouseout="par2net_stop()" onmouseover="par2net_start()">
    <div class="image-container">
      <img src="../images/DataAss.png" width="160" alt="Image">
    </div>
    <div class="text-container">
      <papertitle>Data association between event streams and intensity frames under diverse baselines</papertitle>
      <br>
      Dehao Zhang, Qiankun Ding, <strong>Peiqi Duan</strong>, Chu Zhou, and Boxin Shi<sup>*</sup>
      <br>
      <em>ECCV</em>, 2022
      <br>
      <a href="https://assets.ctfassets.net/yreyglvi5sud/77lslHrshWxDUxmUla9out/b1772a73762d82a0a039fa96d506bbc3/Zhang_ECCV22e.pdf">[paper]</a> 
    </div>
  </div>

   <div class="flex-row" onmouseout="par2net_stop()" onmouseover="par2net_start()">
    <div class="image-container">
      <img src="../images/EvUnroll.png" width="160" alt="Image">
    </div>
    <div class="text-container">
      <papertitle>EvUnroll: Neuromorphic events based rolling shutter image correction</papertitle>
      <br>
     Xinyu Zhou<sup>#</sup>,<strong>Peiqi Duan<sup>#</sup></strong>, Yi Ma, and Boxin Shi<sup>*</sup>
      <br>
      <em>CVPR</em>, 2022
      <br>
      <a href="https://assets.ctfassets.net/yreyglvi5sud/1VoRnvDiIhvSnL0yc7TpAh/4e69c5da94f6d61e3f7172a10edf92b2/Zhou_CVPR22a.pdf">[paper]</a> 
      <a href="https://github.com/zxyemo/EvUnroll">[website]</a>
    </div>
  </div>

  <h2><span>2021</span></h2>
  <div class="flex-row" onmouseout="par2net_stop()" onmouseover="par2net_start()">
    <div class="image-container">
      <img src="../images/GEFpami.png" width="160" alt="Image">
    </div>
    <div class="text-container">
      <papertitle>Guided Event Filtering: Synergy between intensity images and neuromorphic events for high performance imaging</papertitle>
      <br>
      <strong>Peiqi Duan<sup>#</sup></strong>, Zihao W. Wang<sup>#</sup>, Boxin Shi<sup>*</sup>, Oliver Cossairt, Tiejun Huang, and Aggelos K. Katsaggelos
      <br>
      <em>TPAMI</em>, 2021
      <br>
      <a href="https://downloads.ctfassets.net/yreyglvi5sud/7Lv91dkRm8ccSeSj7E8pkb/0e61ecea37676e626ea32e5a2683ad41/Duan_TPAMI21.pdf">[paper]</a> 
      <a href="https://sites.google.com/view/guided-event-filtering/">[website]</a>
    </div>
  </div>

   <div class="flex-row" onmouseout="par2net_stop()" onmouseover="par2net_start()">
    <div class="image-container">
      <img src="../images/EventZoom.png" width="160" alt="Image">
    </div>
    <div class="text-container">
      <papertitle>EventZoom: Learning to denoise and super resolve neuromorphic events</papertitle>
      <br>
     <strong>Peiqi Duan</strong>, Zihao W. Wang, Xinyu Zhou, Yi Ma, and Boxin Shi<sup>*</sup>
      <br>
      <em>CVPR</em>, 2021  [Oral presentation, 4.19%]
      <br>
      <a href="https://assets.ctfassets.net/yreyglvi5sud/MyFOVymXNOWuqYwi5tWMK/5be8c2c2c7f0061de3e5d39979ea3283/Duan_CVPR21.pdf">[paper]</a> 
      <a href="https://sites.google.com/view/EventZoom">[website]</a>
    </div>
  </div>

<h2><span>2020</span></h2>
  <div class="flex-row" onmouseout="par2net_stop()" onmouseover="par2net_start()">
    <div class="image-container">
      <img src="../images/HDRcvpr.png" width="160" alt="Image">
    </div>
    <div class="text-container">
      <papertitle>Neuromorphic camera guided high dynamic range imaging</papertitle>
      <br>
      Jin Han, Chu Zhou, <strong>Peiqi Duan</strong>, Yehui Tang, Chang Xu, Chao Xu, Tiejun Huang, and Boxin Shi<sup>*</sup>
      <br>
      <em>CVPR</em>, 2020
      <br>
      <a href="https://downloads.ctfassets.net/yreyglvi5sud/6AVrg44HPuHRAp3NlzUmtZ/bb1bb60a12c73a12dd8d117dcfdf2422/Han_CVPR20.pdf">[paper]</a> 
    </div>
  </div>

   <div class="flex-row" onmouseout="par2net_stop()" onmouseover="par2net_start()">
    <div class="image-container">
      <img src="../images/GEFcvpr.png" width="160" alt="Image">
    </div>
    <div class="text-container">
      <papertitle>Joint filtering of intensity images and neuromorphic events for high-resolution noise robust imaging</papertitle>
      <br>
     Zihao Wang<sup>#</sup>, <strong>Peiqi Duan<sup>#</sup></strong>, Oliver Cossairt, Aggelos Katsaggelos, Tiejun Huang, and Boxin Shi<sup>*</sup>
      <br>
      <em>CVPR</em>, 2020
      <br>
      <a href="https://downloads.ctfassets.net/yreyglvi5sud/3cjQpBS1ZziYsK5oWHAhW9/775c6d7a2b4a3964c92ea662cd71ef72/Wang_CVPR20.pdf">[paper]</a> 
      <a href="https://sites.google.com/view/guided-event-filtering">[website]</a>
    </div>
  </div>


<style>
    /* Flexbox 容器 */
    .flex-row {
      display: flex;
      align-items: center; /* 垂直居中 */
      padding: 20px;
      /* 可选：添加背景颜色或其他样式 */
      /* background-color: #f9f9f9; */
    }

    /* 图片容器 */
    .image-container {
      flex: 0 0 160px; /* 固定宽度 */
      margin-right: 20px; /* 图片与文字之间的间距 */
    }

    /* 确保图片不带边框且适应容器 */
    .image-container img {
      display: block;
      width: 160px;
      height: auto;
    }

    /* 文字容器 */
    .text-container {
      flex: 1; /* 占据剩余空间 */
    }

    /* 自定义 papertitle 样式 */
    papertitle {
      font-size: 1em;
      font-weight: bold;
    }

    /* 移除默认的表格样式（如果仍在使用表格） */
    table, td {
      border: none;
      padding: 0;
    }
  </style>
